Sistema decimale additivo -> 500 d.C. India
    diffuso: Mohammed ibn-Musa al-Khuwarizmi -> 820 d.C.
    diffuso in europa:  Leonardo Pisano (il Fibonacci) -> 1202 d.C. (Liber Abaci in latino "Libro di calcolo")

Abaco -> 200 a.C. Cina

'600: chiave per la matematica applicata (industria bellica, navigazione transoceanica)
        diffusione: Numeri decimali, Numeri negativi

Invenzione Logaritmi -> 1614, John Napier (Nepero)

Regolo calcolatore -> 1620, Edmund Gunter
    (Moltiplicazioni e divisioni)

Orologio calcolatore -> 1623, Wilhelm Schikard
    (Addizioni e Sottrazioni)

Pascalina -> 1641, Blaise Pascal
    (solo addizioni)

1672-1673, Wilhelm Leibniz:
    - prima macchina calcolatrice
    - sviluppa il sistema di numerazione binario

1725, Basile Bouchon:
    - ideò l'utilizzo di rotoli di carta perforata su telai
1801, Joseph Jacquard:
    -  telaio Jacquard (usa schede perforate metalliche per il controllo di un telaio, in gran parte automatico)
    - schede perforate rimangono in uso per la programmazione dei moderni computer fino intorno al 1980

1822, Thomas de Colmar: aritmometro (macchina calcolatrice portatile in grado di eseguire le 4 operazioni con risultati fino a 12 cifre)

1820-1830, Charles Babbage:
    - Difference engine (dispositivo meccanico in grado di eseguire alcuni tipi di operazioni aritmetiche)
    - Analytical Engine (una macchina in grado di calcolare funzioni arbitrariamente complesse su un numero qualsiasi (ma comunque limitato) di variabili)
    - Babbage attira l'attenzione di  Giovanni Plana spera che voglia scrivere e pubblicare un resoconto delle sue conferenze, incarico che però Plana affida ad un suo allora sconosciuto allievo, Luigi Federico (conte di) Menabrea
    - Ada Augusta Byron, contessa di Lovelace tradusse il lavoro di Menabrea per una rivista inglese
    - Babbage è tradizionalmente considerato l'inventore del primo computer e Ada la prima programmatrice nella storia
    - componenti Analytical Engine:
        * mill: dispositivo che eseguiva le operazioni (untà di controllo)
        * schede: programma che deve essere eseguito dal mill
    - concetto:
        * memoria
        * computazione parallela
        * input e output

1854, George Boole -> inventore dell'Algebra Booleana

Censimento del 1890 -> Herman Hollerith: sistema elettrico di tabulazione (vince il concorso...)
1896: Hollerith fonda la Tabulating Machine Corporation
1924: nascerà la International Business Machine Corporation (IBM)

Schede Perforate: rappresentavano il metodo per rappresentare le informazioni e dovevano essere elaborate in modo automatico
    - furono modificate in modo da poter inserire anche i programmi da eseguire.

'800-'900: progettati alcuni calcolatori analogici (lavoravano con grandezze reali, che variano in modo continuo)

1913, Vannevar Bush -> primo calcolatore analogico generale
    crea il memex (apparecchiatura con cui si possano raccogliere e organizzare testi)

1930 - 1945:
    - i calcolatori analogici erano molto difficili da programmare, quindi si passò ai calcolatori digitali
    - nella nascita dei computer digitali furono importanti l'invenzione di relè e delle valvole termoioniche

1835, Joseph Enry -> inventa il relè (un interruttore fatto scattare con un elettrocalamita)
1906, Lee De Forest -> invenzione del triodo (primo comopnente in grado di amplificare un segnale elettrico)
    (diodo e triodo sono valvole termoioniche)

1937, George Sitbitz ->  Model K, primo computer digitali (utilizzando dei relè)
    1937-1939 progetta e costruisce il Complex Number Calculator (450 relè)
    (4 operazioni con numeri complessi)

~1935, Claude Shannon -> studiò la corrispondeza tra la logica booleana e i cirtuiti digitali a relè

1936-1938, Konrad Zuse -> Z1 (la sua prima macchina basata sulla numerazione binaria)
    (internamente simile ai computer moderni)
    - Memory: 64 word da 22 bit indirizzate dal Memory Selector
    - Arithmetical Unit: con 2 registri
    - Lettore di nastro perforato
    - Control Unit
    - Memory Selector: per indirizzare la Memory
    - Una tastiera per inserire l’input
    - Un dispositivo di output

1937, Howard Aiken -> Mark I (simile all'Analytical Engine di Babbage ma con i relè)
    (utilizzo: 1943-1959)
    - (ricorda l'architettura Harvard: separazione tra memoria dei dati e memoria delle istruzioni)
    - Grazie all'esperienza sviluppata con il Mark I, IBM decise di dedicarsi allo sviluppo di calcolatori elettronici

1945, Grace Murray Hopper -> scopre il primo bug
1947 -> Mark II (costruito con relè elettromagnetici, molto più veloci di quelli elettromeccanici)

1937-1941, Atanasoff-Berry Computer -> ABC (primo computer realizzato completamente con componenti elettronici)
    - (risoluzione di equazioni lineari)

dal 1925, nazisti -> Enigma (usata per criptare i messaggi)
1932, polacchi -> Bomba (decriptare i messaggi dei nazisti)
    - Enigma viene modificata per aumentare il numero di codifiche possibili
1939, Gordon Welchman e Alan Turing -> creano una versione più sofisticata di Bomba

1941, nazisti -> iniziano ad usare la cifratura di Lorenz
1944, Tommy Flower -> Colossus Mark 1 (macchina completamente elettronica)
    - (decodifica i messaggi dei nazisti)

1944 ->  Colossus Mark 2 (5 volte più veloce)

ENIAC:
 - Gli Americani avevano bisogno di calcolare le tavole balistiche in tempi più ragionevoli, si pensò di sostituire tutti i computers umani con un calcolatore elettronico.
 - Herman Goldstine contattò John Mauchly e insieme a John Eckert stipularono un contratto per lo sviluppo dell' Electronic Numerical Integrator and Computer (ENIAC)
 - i lavori terminarono nel 1945, troppo tardi per la guerra
 - non usava ancora un programma memorizzato
 - Veniva programmato riconfigurando opportunamente a mano gli switch e i collegamenti elettrici, e poteva eseguire salti condizionati e cicli.
 - Prendere un problema da risolvere e mapparlo nell’ENIAC era un compito molto complicato, che richiedeva settimane! Tuttavia impiegava pochi secondi per l'esecuzione.
 - quasi tutta la programmazione maunale veniva fatta da 6 donne...
 - l’ENIAC fu spento definitivamente alle ore 23:45 del 2 ottobre 1955

EDVAC:
 - dopo la costruzione dell'ENIAC, Ecker e Mauchly incominciano a lavorare all'EDVAC
 - 1944 -> Ecker DESCRIVE un nuovo tipo di memoria: Mercury Delay Line (MDL)
    * aveva lo scopo di memoriazzare sia dati che programma
    * memoria ad accesso sequenziale
 - von Neumann scrisse un report dal titolo "First Draft of a Report on the EDVAC"
    * contiene la descrizione della struttura logica di un computer basato su programma memorizzato
    * (la distribuzione al pubblico di informazioni sull’EDVAC fece si che l’EDVAC stesso non potesse più essere brevettato)
 - Architettura Eckert-Goldstine-Mauchly-von-Neumann
 -  l’idea di memorizzare i programmi in una memoria, e potenzialmente nella stessa memoria che contiene i dati è ottima:
   * esecuzione dei programmi più veloce
   * cambiare il programma da eseguire o modifica e ricompilazione più facili e veloci
   * istruzioni di controllo più facili da eseguire
 - problemi:
    * necessità di una memoria sufficientemente spaziosa
    * rischi di modificare inavvertitamente il programma
 - 1946-1949 -> costruzione
 - 1951 -> diventa operativo
 - composto da:
    * 1 memoria
    * 3 registri
    * 1 ALU
    * 1 Control Unit
 - informazioni in codice binario (a differenza dell'ENIAC)
    * istruzione: 44 bit
    * 12 istruzioni macchina disponibili
 - non esisteva un concetto di memoria secondaria permanete
 - rimase operativo per ~10 anni
 - sicuramente il primo a contenere l'IDEA di un computer a programma memorizzato, ma non si può considerare tale (vedi SSEM)

 - l'efficienza del computer non dipendeva solo dalla velocità di calcolo, ma anche dalla velocità con cui vengono inseriti di dati

SSEM (Small Scale Ecperimental Machine):
 - 1948 -> divenne operativo
 - Primo computer a programma memorizzato
 - progettato da  F. Williams e T. Kilburn:
    * inizialmente solo per testare un nuovo tipo di memoria (Williams-Kilburn Tube (WKT))
        - un tipo di memoria per computer che eliminava alcuni dei difetti delle Mercury Delay Lines.
 - macchina a 32 bit
 - l'ALU eseguiva solo sottrazioni e negazioni (le altre erano svolte tramite software)
 - aveva 4 memorie WKT utilizzate per:
    * 1 ram
    * 1 registro (contentente istruzioen in eseguzione)
    * 1 registro (accumulatore: riusltato delle operazioni)
    * 1 monitor

Il successo del SSEM, portò rapidamente allo sviluppo del Manchester Mark I.

Alcuni considerano il Whirlwind I, sviluppato in Massachusetts da Forrester e Everett, il primo computer a programma memorizzato. 

In realtà fu completato in seguito al SSEM ma conteneva due fondamentali innovazioni
* fu il primo computer ad operare in parallelo sui 16 bit che componevano le sue word, con l'inserimento di una memoria a nucleo magnetico, core memory
* introdusse il microprogramma, un livello intermedio tra le porte logiche e le istruzioni macchina

- Nel 1952 viene usato un UNIVAC I per predire in diretta i risultati delle elezioni presidenziali.

- Nel 1947, ai laboratori Bell, Bardeen, Brattain, e Shockley, annunciano lo sviluppo del transistor, che sostituirà le valvole termoioniche. 

Con i transistor, si possono costruire circuiti logici molto più veloci, affidabili e compatti, e con consumi minori.
   
Nel 1953 Kilburn costruisce una versione a transistor del Manchester Mark I

LE MEMORIE NEGLI ANNI 50
- vengono sviluppate nuove tecniche di memorizzazione, i nastri magnetici, le rotating drum memory e gli hard disk.

L'UNIVAC I, progettato da Eckert-Mauchly fu uno dei primi computer commerciali.
- usava come memoria principale una MDL, come memoria di massa un nastro magnetico.

- Nel 1952 commercializzazione Mainframe IBM.
Il termine Main Frames, significa grossi armadi in cui erano ospitati la CPU e la memoria del computer. Molto costosi e usati da grandi compagnie.

Erano suddivisi a seconda dell'utilizzo:
* Applicazioni scientifico ingegneristiche
* Applicazioni commerciali e elaborazione dati
I primi modelli utilizzavano WKT memory, poi sostituiti da memorie a tamburo rotante e dalle Magnetic Core Memory (MCM).

Le memorie a nucleo magnetico sono state la forma predominante di memoria primaria all’incirca dal 1955 al 1975, ideate da JAY FORRESTER.

Le prime MCM avevano tempi di accesso intorno ai 10 millisecondi. Erano spesso chiamate core memory o core, ed è per questo che il file che registra un errore di sistema viene chiamato "CORE DUMP".

- L'IBM 650 -> 1953
  
Fu il primo computer ad essere commercializzato in grandi quantità.  
Viene utilizzato per l'insegnamento della programmazione nelle scuole e molti lo considerano l'antenato del PC.  
Funzionava a tubi catodici con memoria a tamburo rotante "Magnetic drum" ed era composto da
* unità principale
* modulo di alimentazione 
* lettore/perforatore di schede
  
I dati erano memorizzati in word contenenti valori decimali a 10 cifre, un meccanismo che non avrà successo in futuro.  
Era composto da 3 registi per contenere:
* l'istruzione in esecuzione 
* il dato indirizzato dall'istruzione
* il risultato dell'esecuzione
  
Le istruzioni macchina avevano un formato fisso: XX YYYY ZZZZ
* xx:   il codice operativo da eseguire
* yyyy: l'indirizzo in memoria del dato da usare
* zzzz: l'indirizzo in memoria della prossima istruzione da eseguire
  
1957 -> diventa disponibile un compilatore per il FORTRAN, primo linguaggio di programmazione, ma questo non rese l'utilizzo del 650 più comodo

1968 -> **DONALD KNUTH** -> The Art of Computer Programming (TAOCP)
Era trattato qualsiasi fondamento teorico e qualsiasi algoritmo
- ordinamento, ricerca, ricorsione, complessità, numeri casuali etc.
  
Knuth non è solo noto per il TAOCP, constatò la necessità di un editore di testi al computer, e sviluppò il TeX.

1956 -> Werner Buccholz inventa la parola byte 
(ci volle un po' perchè la dimensione si stabilizzasse a 8 bit)

- MEMORIE A TAMBURO ROTANTE
L'IBM 650 era dotato di una memoria "Magnetic Drum Memory" (MDM).  
Si incominciarono a costruire computer dotati di due livelli di memoria
primaria
* Memorie a nucleo magnetico (MCM)
* Memorie a tamburo rotante  (MDM)
Sia le MDM che le MCM erano memorie non volatili, ma troppo piccole per essere usate come memoria secondaria, per la quale si continuarono ad usare nastri magnetici e hard disk

1932 -> GUSTAV TAUSCHEK inventa le MDM
Costituite da un tamburo rotante la cui superficie era ricoperta da bande di materiale ferromagnetico.
Con l'inizio degli anni 60 le MDM furono velocemente rimpiazzate dalle MCM e dagli hard disk

1956 -> Primo Hard Disk
Concettualmente molto simile a quelli moderni, con capacità e dimensioni diverse

1958 -> JACK KILBY, primo circuito integrato (IC)
Conteneva un solo flip flop e pochi transistor, ma aveva dimostrato che questi componenti si potevano integrare e miniaturizzare

I primi IC contenevano ben pochi transistor, col tempo si è riusciti
ad inserirne sempre di più su un’unica fettina di silicio, fino a
realizzare processori multi core formati da qualche miliardo di transistor. 

La LEGGE DI MOORE stima che il numero di transistor inseribili in un IC raddoppia ogni 18/24 mesi

GLI ANNI '60 E LA NASCITA DEI COMPUTER MODERNI:
- **PDP-1**

    Commercializzato nel 1960 è importante per la storia dell'informatica perchè detiene alcuni primati:
    
    - Primo computer ad usare un monitor
    - Primo computer ad ospitare un videogame (spacewar)
    - Primo computer su cui sia stato sviluppato un **text editor**, un **debugger** e un **word processor**.

    Su di esso si iniziò a sviluppare la cultura **hacker**.

    - Caratteristiche:

        - Era composto da un lettore di nastro perforato come memoria di massa e come MCM.
        - Era in grado di eseguire 100k istruzioni al secondo.
        - L'output veniva scritto sul nasteo perforato o sul monitor.
        - Era completamente costruito con transistor e diodi montati su schede.

- **ILIAC II**

    Compare la prima idea di **pipeline** -> fetch, decode, excecute.

- **IBM 7030**

    Oltre alla pipeline, anche **protezione della memoria**, **byte da 8 bit** e **memoria interlacciata**.

- **ATLAS**

    Implementate la **paginazione e la memoria virtuale**.


La serie IBM/360, a partire dal 1964 inaugura la terza generazione di computer, basata sui CIRCUITI INTEGRATI

(seconda generazione -> costituiti da transistor)
(prima generazione -> valvole termoioniche)

Novità 
* L'adozione di byte di dimensione fissa da 8 bit
* Memoria principale indirizzabile a byte
* Word di dati da 32 bit
* L'uso esteso del microcodice in tutti i processori
* 16 registri da 32 bit ciascuno
* Una program status word da 64 bit
* Un meccanismo di interrupt
* L'uso di un tlb per la traduzione degli indirizzi
* Istruzioni di lunghezza variabile
* Istruzioni macchina complesse con 2 operandi 
* Introduzione del Direct Memory Access (DMA)


- il CDC 6600
  
SEYMOUR CRAY è una figura leggendaria nel campo dei supercomputer. 

Progetta il **CDC 6600**, considerato il primo supercomputer della storia, il più veloce della sua generazione.

Crey introdusse il **Processore Periferico**, che operando in parallelo al processore principale, lo liberava della gestione diretta dei vari dispositivi.

Non più oberata di lavoro quindi, la CPU del CDC 6600 era decisamente la più veloce di quel tempo

Su questa falsariga, la CPU oggi è assistita da altri processori, il NORTHBRIDGE, per comunicare con i dispositivi veloci, e il SOUTHBRIDGE per comunicare con i dispositivi più lenti

Questi circuiti integrati, detti **CHIPSET**, sono collegati alla cpu motherboard del computer

Il processore era dotato di 10 unità funzionali che potevano eseguire istruzioni in parallelo. Primo esempio di INSTRUCTION LEVEL PARALLELISM, oggi usato da tutti i processori moderni

- 1964 -> Douglas EngelBart inventa il **MOUSE**

- 1965 -> PDP-8, nasce una nuova categoria, i MINICOMPUTER
  

LE PRIME CACHE
1965 -> Maurice Wilkes
Idea di utilizzare uno strato di memoria intermedio tra RAM e CPU che sia più veloce, la **CACHE**.

Il primo computer a contenere una cache fu l'IBM 360/85. Al giorno d'oggi tutti i processori usano più livelli di cache.

- 1967 -> Robert Tomasulo inventa nuovo schema architetturale
OUT OF ORDER, le istruzioni vengono eseguite in ordine diverso da quello con cui le ha generate il compilatore

Lo schema si basa sulle RESERVATION STATION (RS), complesse memorie in cui sostano le istruzioni in attesa di essere eseguite.
Appena l'operando diventa disponibile, viene smistato attraverso il COMMON DATA BUS (CDB).

- PDP 11 -> Anni 70
Il computer più famoso della Digital Equipment Corporation. 
La caratteristica fondamentale era l'ortogonalità dell'instruction set
* qualunque modalità di indirizzamento dei dati poteva essere usata da qualsiasi istruzione
  
- Floppy Disk -> 1971
Erano dei semplici supporti di memoria magnetica permanente, trasportabili.

- INTEL -> 1968, ROBERT NOYCE e GORDON MOORE
Fondano la NM electronics, che diventerà INTEL (integrated electronic corporation)

ANDREY GROVE -> assunto dalla INTEL, la farà diventare il più importante produttore di circuiti integrati

Nel 1966 la Transitron produce un TTL IC in grado di memorizzare 16 bit di dati, nel frattempo al IBM viene sviluppata l'idea di Dynamic Ram, usando un solo transistor per memorizzare i dati

1969 -> La intel produce il 3101 Schottky bipolar memory, un innovativo chip  
1971 -> la prima Eprom, la 1702
1979 -> Intel 1103, Magnetic Core Memory Killer, grande quantità di Ram in un unico chip

1970 -> la Intel assume Federico Faggin per lo sviluppo di un nuovo chip.
1971 -> La Intel rilascia il chip, chiamato 4004, il primo processore sviluppato su un'unica fettina di silicio.

Insieme al 4004 svilupparono:
* 4001: ROM
* 4002: RAM
* 4003: input/output

- COMPUTER TERMINAL CORPORATION, poi DataPoint ->1968, Ray e Roche
A quei tempi, per interagire con i computer si scriveva su carta e di conseguenza i dispositivi erano lenti e rumorosi (TELETYPE Model 33 -> usata con i computer IBM)

DataPoint 3300 -> nuovo terminale che poteva sostituire la Teletype 33
Molto più silenzioso e veloce, dotato di monitor
Il successo del 3300 sollevò enorme richiesta di terminali simili per altri modelli di computer

1969 -> Poor e Pyle vengono assunti con il compito di creare un computer per qualunque terminale scrivente

Le intenzioni di Poor erano di utiizzare un centinaio di Transistor, invece Ray e Roche invitano Poor a parlare con la Intel per realizzare ilprogetto usando un unico circuito integrato

La Intel era impegnata nel 4004, e quindi il prototipo viene sviluppato usando normali circuiti integrati

1970 -> esce quindi il DataPoint 2200, come terminale programmabile
Visto il successo, viene sviluppato il 2200 II, molto più potente


**Gli anni '70: microprocessori**

La capacità di inserire più componenti miniaturizzati su un'unica fettina di silicio, permetteva di creare processori sempre più prestanti:

* **Velocità**: lavorare con frequenze di clock più alte. (da 0,8 MHz a circa 4 GHz)
* **Sofisticati**: sfruttare tecniche diverse per velocizzare l'esecuzione di istruzioni.
* **Più memoria a disposizione**: RAM e più livelli di cache (da qualche KB a qualche GB).

Integrare più transistor porta una grande evoluzione:

* Anni '60: **centinaia** di transistor
* Anni '70: **migliaia** di transistor
* Anni '80: **centinaia di migliaia** di transistor
* Anni '90: **milioni** di transistor
* Nel 2005 viene prodotto il primo chip contenente più di un **miliardo** di transistor

Come dimensioni siamo all'incirca nell'ordine dei micro/nano metri. Era nota usare la terminologia **"tecnologia a xx micro/nano metri"** introdotta dalla **International Technology Roadmap for Semiconductors**.

Tra le **innovazioni architetturali** troviamo:

* L'evoluzione **RISC** -> anni '80: istruzioni macchina riviste in modo da poter creare macchine più semplici e veloci.
* Lo sfruttamento dell' **Instruction Level Parallelism** -> anni '90.
* Introduzione del **multithreading** e dei processori **multicore** -> fine anni '90 / inizio 2000.
* Uso dei processori grafici per elaborazioni non grafiche (es. GPGPU, GPGP ecc..).

**Xerox** -> nel 1973:

Viene sviluppato **Alto** il primo desktop/personal computer con mouse, GUI e collegamento a rete LAN tramite ethernet.

Ispirato alle idee di Engelbart (inventore del mouse) era un minicomputer ma considerato a tutti gli effetti un **PC**. (influenzò i progetti del Macintosh e delle workstation Sun)

**Apple** -> 1976, Steve Jobs e Steve Wozniak

Wozniak aveva l'intenzione di trasformare il terminale video progettato da lui stesso in un vero e proprio computer.

A quei tempi però, come microprocesori disponibili c'erano l'Intel 8080 e il Motorola 6800 troppo costosi per essere utilizzati. Poco dopo viene reso disponibile il 6502, un microprocessore super-economico molto simile al Motorola 6800, così Wozniak monta quello sul suo progetto.

Alla presentazione del progetto, Jobs riesce a trovare un acquirente disposto a comprare qualche esemplare e da lì venne venduto come **Apple I**.

Ai due (geni indiscussi), si aggiunge un terzo socio Mike Markkula e insieme fondano la **Apple Computers Inc.** (1 aprile 1976)

Intanto, Wozniak inizia a progettare l'**Apple II** che sarà presentato nel 1977.

Nel 1980, venne rilasciato l'**Apple III**, simile al II, ma senza ventilatore. Questo costrinse l'azienda a ritirare dal mercato 14k esemplari.

**Il PC IBM**

Nel 1981 la IBM rilascia l'IBM 5150, un desktop computer con Intel 8080, RAM da 256KB e Floppy come memoria di massa.

**PC vs MAC**

Nel 1979, dopo una visita di Apple a Xerox Parc, nasce l'idea di creare un nuovo computer con UI, icone e finestre.

Inizialmente l'idea viene sviluppata sul **LISA**, con scarso successo per il costo troppo elevato, ma subito dopo con l'uscita del **Macintosh 128k** ebbe un grande successo. Questo segnò l'inizio della divisione tra **PC e MAC**.

**RISC Revolution** -> anni '80 (Restricted Instruction Set Computer)

Patterson e Hennessy diedero luogo alla RISC Revolution, nuovi processori caratterizzati da istruzioni macchina semplici, struttura regolare e dimensione fissa.

Le nuove macchine RISC, rimpiazzeranno i vecchi processori denominati CISC. (Complex Instruction Set Computer)

Negli anni '60 e '70 i processori impiegavano instruction set con le seguenti caratteristiche:

* Gran numero di istruzioni macchina
* Istruzioni macchina complesse
* Molteplici modalità di indirizzamento della memoria dati
* Possibilità per tutte le istruzioni di indirizzare la memoria dati usando una delle tante modalità

(es. istruzione MVC usata per spostare dati in vari punti della RAM)

Le istruzioni macchina complesse avevano dei vantaggi:
* Generavano eseguibili molto ridotti
* Dato che i tempi di accesso alla memoria principale erano elevati, aveva senso esprimere istruzioni con grande quantità di lavoro
* I compilatori erano arretrati, erano progettati per questo tipo di operazioni.

Parte della difficoltà della traduzione veniva trasferita al processore, grazie al microprogramma.
* L'uso della microprogrammazione portava alla generazione di istruzioni complesse
* Essendo memorizzate nella ROM, era comodo aggiungere il relativo microprogramma alla ROM
* Era possibile cambiare l'instruction set solo cambiando la ROM.

Tuttavia aveva delle conseguenze:
* Richiedevano molti byte di descrizione, le istruzioni avevano lunghezza variabile.
* La lunghezza variabile faceva si che i bit per gli operandi si trovassero in posizioni diverse

Istruzioni così formate avevano tempi di esecuzione elevati:
* La memoria istruzioni veniva letta a blocchi di dimensione fissa, quindi non era possibile stabilire se era stata presa un'istruzione completa, o due ecc..
* Una volta prelevata l'istruzione era necessario decodificare l'opcode per capire in quali bit venivano specificati gli operandi
* Istruzioni complesse richiedono data-path e UC complesse
* Poichè tutte le istruzioni possono indirizzare la RAM, poteva nascere un bottle-neck

Per ovviare a queste problematiche (negli anni '80) si iniziò a studiare un nuovo paradigma architetturale:
* A Berkeley, Patterson e Sequin creano un nuovo processore chiamato **SPARC**
* A Stanford, Hennessy sviluppa un processore chiamato **MIPS**

Un'architettura RISC quindi, impiega istruzioni macchina **semplici** ovvero che specificano poco lavoro

Quindi:
* Data-path corto e cicli di clock più corti
* Istruzioni tutte con la stessa lunghezza e struttura regolare
* Solo istruzioni di **LOAD e STORE** indirizzano la RAM (no bottle-neck)

**MIPS** -> anni '80, Hennessy (Microprocessor without Interlocked Pipelines Stages)

**SPARC** -> anni '80, Patterson e Sequin (Scalable Processor ARChitecture)

RISC  quindi per funzionare aveva bisogno di RAM più grandi e più registri.

A causa della diatriba tra i sostenitori RISC e CISC, vennero confrontati la prima volta nel 1989 tra il VAX 8700 e il MIPS M2000. **MIPS era tre volte più veloce del VAX!**

Di conseguenza dagli anni '90 tutti i produttori adottarono l'architettura RISC, tranne la INTEL che usa una variante di CISC.

**ILP** -> anni '90 (Instruction Level Parallelism)

Sfruttamento del parallelismo delle istruzioni in esecuzione:
* **Scheduling dinamico della pipeline**: le istruzioni indipendenti vengono eseguite in qualsiasi ordine
* **Branch prediction dinamico**: l'esito dei salti condizionati viene predetto in base alle esecuzioni precedenti
* **Speculazione hardware**: l'esecuzione di istruzioni prima di sapere se effettivamente dovranno essere eseguite
* **Multiple issue**: avvio all' esecuzione più istruzioni in parallelo
* **Ottimizzazione del compilatore**: produrre codice più "sfruttabile" dalla CPU per l'esecuzione parallela

**CPU MULTITHREAD e MULTICORE** -> anni 2000
* I processori **multithread** eseguono in parallelo istruzioni appartenenti a più peer thread
* I processori **multicore** permettono di eseguire in parallelo più processi

    **Si combinano le due tecniche in modo che ogni core implementi il multithread**

**GPU** -> anni 2000
* Le GPU possono elaborare in parallelo grandi quantità di dati in virgola mobile
* I supercomputer di oggi sono composti da milioni di core e combinano CPU multi-core e GPU per poter computare migliaia di **TeraFLOPS** (FLoating OPeration per Second).


-------------------------------------------------------------------------------------------------


**STORIA DEI LINGUAGGI DI PROGRAMMAZIONE**

Non è possibile stabilire una data precisa in cui comincia la storia dei linguaggi di programmazione.

**Charles Babbage** derivò l'idea di pilotare l'Analytical Engine tramite le schede perforate (come nel telaio di Jaquard)

**Ada Lovelace** -> 1943 elaborò una specifica per calcolare i numeri di Bernoulli, per questo verrà considerata la **prima programmatrice della storia**

**Zuse** -> 1943/45 concepì per lo Z3 il **Plankalkul**, una sorta di linguaggio ad alto livello

**Mark I** -> (1943, Haiken) le istruzione erano codificate su nastro perforato e i loop erano ottenuti incollando insieme l'inizio e la fine del nastro.

**Goldstine e Von Neumann** sviluppano l'idea di Flow Diagrams, adesso meglio noti come **Flowchart** e inizia a diffondersi il concetto informatico di **assegnamento**

**SSEM** i dati venivano inseriti a mano, utilizzando 32 switch per stabilire il valore di ogni bit.

**EDVAC** i bit che componevamo il programma, erano inseriti singolarmente nella MDL (Mercury Delay Lines).

**EDSAC** (1949, Renwick, Wilkes)
Fu il secondo computer a programma memorizzato, ma il primo ad essere usato per scopi commerciali
E' formato da:
* Tubi catodici
* MDL da 512 words
* 2 registri
* Un lettore di nastro perforato per l'input
* Telescrivente per l'output

Viene introdotta per la prima volta l'idea di dare alle istruzioni macchina (**Initial Orders**) una forma mnemonica, quindi più utilizzabile dagli essere umani

Al codice operativo era associato un numero a due cifre che indicava l'indirizzo in memoria dell'operando.

Il programma andava codificato sul nastro usando un perforatore manuale

In sostanza, gli Initial Orders rappresentavano una **forma primitiva di Assembler**

Nasce anche il concetto di **subroutine** che erano porzioni di codice richiamabili. Furono ideate da **Wheeler**

**ShortCode di Mauchly**

Mauchly concepisce il primo linguaggio di programmazione ad alto livello lo Short Order Code

Viene **implementato da Shmitt**, un lavoratore della Eckert-Mauchly Computer Corporation (EMCC)

Inizialmento fu concepito per il BINAC, ma fu usato sull'**UNIVAC I**

Il **primo corso di programmazione** fu tenuto da Grace Hopper e i suoi collaboratori intorno al 1949.

**Shmitt** lavorò anche alla scrittura del programma che interpretava le equazioni da calcolare, questo fu il **primo interprete della storia**

**Haskell Curry** sviluppa alcune idee di programmazione strutturata, illustra alcuni algoritmi ricorsivi per convertire la notazione matematica in linguaggio macchina.
Questi algoritmi sono considerabili come il primo esempio di descrizione della fase di **generazione del codice di un compilatore**.

**Rutishauer** (1949/1951) inventa il **ciclo FOR**
Egli usava codici numerici specifici per ogni carattere alfanumerico, questo era **rilocabile**

**Corrado Bohm** (1950) definisce un linguaggio ad alto livello e un metodo di traduzione da livello alto a linguaggio macchina
Introduce:
* il **goto**
* **if-then-else**
* **assegnamento**
* **subroutine**
* La definizione completa di un **compilatore scritto nello stesso linguaggio dei programmi da compilare**

Il compilatore di Bohm si dimostro più efficace di quello di Rutishauer, ed egli dimostro anche che **il suo linguaggio è universale, ovvero capace di calcolare qualsiasi funzione computabile**

**AUTOCODE** (1952, Glennie) fu il primo compilatore effettivamente operativo (fu sviluppato per il Mark I)

Glennie era consapevole che **si potesse usare lo stesso computer per tradurre il codice ad alto livello e far girare il programma tradotto**

Glennie era però troppo "avanti" perchè prima c'erano diversi problemi da risolvere ad esempio:
* Capire perchè l'esecuzione di un programma fallisse
* Risolvere i problemi di scarsa memoria

Il termine linguaggio di programmazione non era ancora in uso, si parlava di **pseudo code**

Grace Hopper sviluppa un sistema in cui alcune pseudo istruzioni possono essere tradotte in linguaggio macchina, lei lo chiama **compiling routine**

Anni 1954-1956:
* Viene sviluppato il primo assembler chiamato **SOAP**
* Viene sviluppato il **BACAIC**, per tradurre espressioni algebriche il linguaggio macchina
* **Elsworth** lavora alla traduzione di equazioni in linguaggio macchina e lo chiama **Kompiler**
* **Blum** presenta **ADES** il primo linguaggio dichiarativo fondato sulla teoria della ricorsione di **Kleene**
* **Perils e Smith** sviluppano un linguaggio e compilatore chiamato **IT**:
    * Il primo linguaggio potente ed espressivo e con una implementazione efficiente
    * Grazie a esso era possibile sviluppare compilatori anche per computer poco potenti (IBM 650)

**FORTRAN** (1957, Backus, Herrick, Ziller)

Esce il linguaggio più famoso e celebrato di quegli anni il FORTRAN.
Permetteva di utilizzare nomi di variabili composti da due lettere.
Il report del FORTRAN era il primo tentativo di specificare in modo rigoroso la sintassi di un linguaggio, in esso iniziò a manifestarsi il concetto di **BNF**

Inizialmente era pieno di errori e parti incomplete, ma quasi un anno dopo fu utilizzato per un buon numero di programmi.

Fu il primo linguaggio di programmazione ad alto livello dotato di un compilatore efficiente e influenzò i linguaggi successivamente creati.

**LISP** (1958, John McCarty)

E' il linguaggio di programmazione più vecchio ancora in uso, dopo il FORTRAN.

E' quello che oggi chiameremmo linguaggio di programmazione funzionale, con molte idee innovative
* uso della ricorsione
* uso di liste concatenate e alberi
* allocazione dinamica della memoria
* possibilità di comporre funzioni in funzioni complesse

La scelta di adottare la notazione (+ 20 30) rendeva i programmi LIPS sorprendentemente efficienti

Ebbe enorme successo in ambiente accedemico e divenne il linguaggio di riferimento per l'AI. Ne sono state sviluppate diverse varianti
* Franz LISP
* Common LISP
* Scheme

**Il COBOL** (1959, consorzio CODASYL)

Linguaggio concepito per applicazioni in campo amministrativo e commerciale. 
Il department of Defense USA obbligò le compagnie di computer a fornire il cobol nelle loro installazioni, cosi questo si diffuse velocemente.

* buona efficienza
* facilità d'uso
* portabilità dei programmi su sistemi diversi

Negli anni '70 era di gran lunga il linguaggio più usato al mondo, ma il futuro non sembra molto promettente.

**L'ALGOL 60** (1960)
 L'algol 58 è ricordato principalmente per la sua influenza sull'Algol 60
 **TONY HOARE**, inventore del QuickSort, osservò che l'Algol era un linguaggio molto avanzato..

 Non era però particolarmente adatto per le applicazioni commerciali.
 Vengono definite per la prima volta le modalità di passaggio dei parametri
 * call by value: l'espressione passata come argomento viene valutata e il risultato viene legato al parametro formale corrispondente nella procedura
 * call by name: una variante del call by reference, poi abbandonata
 * call by reference: si passa un riferimento alla variabile usata come argomento (il suo indirizzo)

L'ALGOL è il primo linguaggio a richiedere la dichiarazione del tipo delle variabili usate nei programmi con parole riservate: 
* INTEGER, BOOLEAN, REAL, DOUBLE, LONG, ARRAY
* Compare per la prima volta l'**if then else**
* Compaiono i blocchi di istruzioni delimitati da begin--end in cui è possibile dichiarare funzioni e variabili locali

Un'altra innovazione introdotta con l'ALGOL è l'uso di una notazione formale per descrivere il linguaggio, la **Backus Naur Form** (BNF). Di fatto è una notazione per grammatiche **context free**, infatti è usata sia per la descrizione della sintassi, sia per il formato dei documenti e protocolli di comunicazione.

Fu sviluppata da **Backus** per descrivere l' ALGOL 58, poi **Naur** la modificò per adattarla all' ALGOL 60 proponendo il cambio nome in **Backus Normal Form**.

Le **grammatiche context free** furono definite da **Chomsky** nella metà degli anni '50.

Il progetto dell'ALGOL portò alla ricerca di tecniche di parsificazione e allo sviluppo del concetto di **parsificatore LL**, in grado di parsificare linguaggi generati da un sottoinsieme delle grammatiche context free.

Nel 1965 **Knuth** definì il **parsificatore LR** in grado di parsificare i linguaggi da qualsiasi grammatica context free.

**BASIC** -> (1964, Kemeny e Kurtz) -> Beginner's All-purpose Symbolic Instruction Code

In realtà furono un gruppo di studenti guidati da Kemeny e Kurtz a sviluppare il compilatore.

Lo scopo del BASIC era di permettere a studenti non esperti o specializzati di utilizzare i computer e scrivere programmi complessi.

Nello stesso periodo iniziarono a diffondersi i sistemi **time sharing**, accessibili tramite **telescriventi remoti**, questo permetteva l'aumentare di utenti.

La vera esplosione del linguaggio avvenne intorno agli anni '70, con la diffusione dei **microcomputer** (computer con microprocessori, economici e quindi alla portata di molti).

Così dalla metà degli anni '70, qualsiasi computer forniva un compilatore BASIC. In alcuni casi, l'interprete dei comandi era BASIC, poteva ricevere istruzioni BASIC direttamente da terminale.

Funzionavano così **APPLE II e il Commodore PET 2001**.

Il BASIC era **sufficientemente leggero** da poter girare su computer scarsi, ma allo stesso tempo ad alto livello e semplice.

Il **BASIC COMPUTER GAME di Ahl** (1978) fu il primo libro libro di informatica a superare il milione di copie vendute.

Ovviamente al linguaggio furono applicate varie migliorie, soprattutto all'ingresso dei PC IBM sul mercato. Il culmine di questo processo si ebbe probabilmente con il **Visual BASIC** (1991) sebbene si trovasse già a competere con linguaggi più avanzati come C e C++.

**Altair 8800** -> (1974, MITS) -> **Home Computer**

Bill Gates e Paul Allen contattano l'azienda per offrire un interprete BASIC.

Allen e Gates fondano la **Micro-Soft** (4 aprile 1975) il cui primo prodotto sarà l'**Altair Basic**.

All'inizio degli anni '70, Wozniak utilizzava il BASIC per scrivere videogiochi. Il suo **GAME BASIC** fu poi ridenominato **Integer BASIC**.

Esso fu adottato come interprete e ambiente di sviluppo nell' Apple I e II, fino a quando la Apple non acquistò una versione completa che chiamerà **Applesoft BASIC**.

**Spaghetti Code**

Sebbene l'ALGOL contenesse primitive di controllo per il flusso, il GOTO permetteva uno stile di **programmazione non strutturata**

Il codice non strutturato prende il nome di **Spaghetti Code**

**Teorema di Bohm Jacopini** (1966)

Dimostrano un teorema che asserisce che qualsiasi funzione computabile può essere calcolata da un programma costituito esclusivamente da:
* **Sequenze** di istruzioni eseguite una dopo l'altra
* Istruzioni di **selezione** (if then else)
* Istruzioni di **iterazione** (while do)

Sostanzialmente il teorema dimostra che si possono scrivere programmi senza il GOTO, ovvero evitando di saltare dentro e fuori a blocchi di istruzione in modo disordinato. Anche Dijkstra critica l'uso del GOTO e dice che dovrebbe essere abolito.

Per molti il GOTO resta comuqnue utile ad esempio:
* In un articolo, Structured Programming with Go To Statement, Knuth mostra che il costrutto è ideale in alcuni casi.
* Nel manuale sul C, Kernighan e Ritchie osservano che il costrutto è fondamentale per gestire condizioni di errore e uscire rapidamente da cicli.




















